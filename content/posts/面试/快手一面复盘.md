---
title: "快手一面复盘"
date: 2020-08-08T17:13:31+08:00
description: ""
draft: true
tags: []
categories: []
---


1. 基于linux内核有做过哪些方面的工作?

2. linux内核有哪些子系统? 对哪个子系统比较熟悉?

3. linux内核内存分配策略? 假如申请128MB,系统会分配多少内存

4. 整个系统由1G的内存,可以用malloc分配1G以上的内存吗?

5. malloc申请之后会不会不分配物理内存,只是分配虚拟地址空间?

6. libc的库,还知道其他的内存库吗?  tc_malloc tp_malloc有听过吗?

7. 你对cache怎么理解的

8. l1 l2 l3cache 的区别

9. um机器的l1 l2 l3的区别? l1和l2有差异吗?  
10. 理解超线程概念吗?l2是超线程之间可以共享的
11. 磁盘的page cache回收时怎么回收(lru,回收的时机?)
12. linux内存去向哪里?(应该通过某个命令行查看系统统计)
13. slab和伙伴系统是什么关系
14. 怎么使用slab
15. kmalloc和kvmalloc和vmalloc
16. 做过哪些驱动,什么类型的
17. 自己写一个字符驱动的需要哪些步骤
18. 上下文切换什么时候需要进行切换
19. 上下文切换大概需要多少时间
20. io uring做了什么改进
21. io uring对上下文切换有什么影响吗?怎么减少切换次数
22. io uring在哪个版本支持
23. 更换内核吗
24. 协程有了解过吗
25. 协程一般用在什么场合
26. 协程和线程有什么优势
27. 虚拟机迁移
28. kvm qemu k8s docker
29. bio起什么作用
30. 文件系统有了解吗
31. 性能优化有了解吗
32. ebpf怎么看
33. 写过bcc工具吗
34. libbpf是干什么的
35. system tab也可以加一些hook
36. system tab和bpf有什么相比
37. bcc用python有什么缺陷吗

> 调度这一块


> tcp网络协议栈和网络基础知识

**通过socket发送数据：**

应用层：将数据传输给tcp/udp层
1. `write(socket,buffer,length)` 或者 `writev(socket,iovector,vectorlen)`
2. 系统调用`sock_sendmsg`
3. `__sock_sendmsg`根据`proto_ops`调用`tcp_sendmsg`或者`udp_sendmsg`

tcp/udp层：拷贝数据、添加头部信息（序列号）、流量控制和拥塞控制
1. 创建`struct sk_buff`，拷贝用户数据到内核空间
2. `tcp_transmit_skb`添加`tcp`或者`udp`头部、序列号等信息
3. `queue_xmit`：将`sk_buff`加入到发送队列

ip层：添加ip头、维护路由表、TTL、分段
1. ip层首先添加ip头，然后检查路由表和维护TTL，如果发送到网络则调用`ip_queue_xmit`，否则将该数据转发到上层协议

数据链路层：添加以太网头部、arp
2. `dev_queue_xmit`

物理层：网卡驱动

> cgroup有了解吗

cgroups 是Linux内核提供的一种可以限制单个进程或者多个进程所使用资源的机制，可以对 cpu，内存等资源实现精细化的控制，目前越来越火的轻量级容器 Docker 就使用了 cgroups 提供的资源限制能力来完成cpu，内存等部分的资源控制。

另外，开发者也可以使用 cgroups 提供的精细化控制能力，限制某一个或者某一组进程的资源使用。比如在一个既部署了前端 web 服务，也部署了后端计算模块的八核服务器上，可以使用 cgroups 限制 web server 仅可以使用其中的六个核，把剩下的两个核留给后端计算模块。

> 中断 系统调用  可延迟函数 做系统资源互斥

采用自旋锁。

1. 在单cpu，不可抢占内核中，自旋锁为空操作。
2. 在单cpu，可抢占内核中，自旋锁实现为“禁止内核抢占”，并不实现“自旋”。
3. 在多cpu，可抢占内核中，自旋锁实现为“禁止内核抢占” + “自旋”。

NOTE：禁止内核抢占只是关闭“可抢占标志”，而不是禁止进程切换。显式使用schedule或进程阻塞（此也会导致调用schedule）时，还是会发生进程调度的。

> 中断上下部分

中断会打断内核中进程的正常调度和运行，调用中断处理函数。在大多数的系统中，当中断到来时可能耗费大量的时间对它进行处理。如果在中断处理函数中没有禁止中断，该中断处理函数执行过程中仍有可能被其他中断打断。所以我们当然希望中断处理函数执行得越快越好。为了在中断执行**时间尽可能短**的和**中断需要处理完大量的工作**之间找一个平衡点，将中断分为上下两部分。

1. 顶半部完成尽可能少比较紧急的任务，它往往是简单的**读取寄存器中的中断状态**并**清除中断标志**后就进行“**登记中断**”的工作。登记中断：也就是将底半部处理程序挂在底半部执行队列中去。这样上半部执行的速度就会很快，可以服务更多的中断请求。

2. 现在，中断处理工作的重心就落在了底半部的头上，它来完成中断事件的绝大多数任务。底半部几乎做了中断处理程序所有的事情，而且可以被新的中断打断，这也是底半部和顶半部的最大不同，因为顶半部往往被设计成不可中断。底半部则相对来说并不是非常紧急的，而且相对比较耗时，不在硬件中断服务程序中执行。 

> 中断下部分一般怎么做? 软中断 tasklet(可延迟函数) workqueue

可延迟函数：软中断和tasklet(tasklet是基于软中断之上实现的)；

**软中断：**

1. 产生后并不是马上可以执行，必须要等待内核的调度才能执行。软中断不能被自己打断，只能被硬件中断打断（上半部）（中断优先级的关系）;
2. 可以并发运行在多个CPU上（即使同一类型的也可以）。所以软中断必须设计为可重入的函数（允许多个CPU同时操作），因此也需要使用**自旋锁**来保护其数据结构。

**tasklet：**

由于软中断必须使用可重入函数，这就导致设计上的复杂度变高，作为设备驱动程序的开发者来说，增加了负担。而如果某种应用并不需要在多个CPU上并行执行，那么软中断其实是没有必要的。因此诞生了弥补以上两个要求的tasklet。它具有以下特性：
1. **一种特定类型**的tasklet只能**运行在一个CPU上**，不能并行，只能串行执行;
2. **多个不同类型**的tasklet可以并行在多个CPU上;
3. 软中断是静态分配的，在内核编译好之后，就不能改变。但tasklet就灵活许多，可以在**运行时改变**（比如添加模块时）。

```c
// linux内核目前使用的软中断
// /kernel/softirq.c
const char * const softirq_to_name[NR_SOFTIRQS] = {
	"HI", "TIMER", "NET_TX", "NET_RX", "BLOCK", "IRQ_POLL",
	"TASKLET", "SCHED", "HRTIMER", "RCU"
};

// HI：处理高优先级的tasklet
// TIMER：和时钟中断相关的tasklet
// NET_TX：把数据包传送到网卡
// BLOCK：
// TASKLET：处理常规的tasklet
// SCHED
// HRTIMER：高精度定时器中断
// RCU

// tasklet动态使用
void tasklet_init(struct tasklet_struct *t, void (*func)(unsigned long), unsigned long data)
```

**workqueue：**

1. **可延迟函数运行在中断上下文**中（软中断的一个检查点就是do_IRQ退出的时候），于是导致了一些问题：软中断不能睡眠、不能阻塞。由于中断上下文出于内核态，没有进程切换，所以如果软中断一旦睡眠或者阻塞，将无法退出这种状态，导致内核会整个僵死。但可阻塞函数不能用在中断上下文中实现，必须要运行在进程上下文中，例如访问磁盘数据块的函数。因此，可阻塞函数不能用软中断来实现。但是它们往往又具有可延迟的特性。
2. 在2.6版的内核中出现了在内核态运行的**工作队列（运行在进程上下文）**（替代了2.4内核中的任务队列）。它也具有一些可延迟函数的特点（需要被激活和延后执行），但是能够能够在不同的进程间切换，以完成不同的工作。可延迟函数：软中断和tasklet(tasklet是基于软中断之上实现的)；
