---
title: "快手一面复盘 - 内核组"
date: 2020-08-08T17:13:31+08:00
description: ""
draft: true
tags: [面试]
categories: [面试]
---

> 基于linux内核有做过哪些方面的工作?

> linux内核有哪些子系统? 对哪个子系统比较熟悉?

1. 进程管理：进程创建、进程调度
2. 系统调用：
3. 中断管理：注册中断、上半部和下半部、中断上下文、中断控制	
4. 内存管理：页、区、kmalloc、vmalloc、伙伴系统、slab、虚拟内存
5. 虚拟文件系统：
6. 块IO：块设备、bio、请求队列、I/O调度
7. 设备与模块：
8. 网络：

> linux内核内存分配策略? 假如申请128MB,系统会分配多少内存

1. malloc分配小于128KB时，采用sbrk函数移动堆顶，内存分配在堆区；
2. malloc分配大于128KB时，采用mmap系统调用，内存分配映射区；
3. 无论malloc通过sbrk还是mmap实现，分配到的内存只是虚拟内存，而且只是虚拟内存的页号，代表这块空间进程可以用，实际上还没有分配到实际的物理页面。

> 整个系统由1G的内存,可以用malloc分配1G以上的内存吗?

主要是看进程的虚拟地址空间，实际的物理地址空间不够的话可以采用请求分页，将内存暂存到外存，需要时调入；

> malloc申请之后会不会不分配物理内存,只是分配虚拟地址空间?

会的；

> libc的库,还知道其他的内存库吗?  tcmalloc tpmalloc有听过吗?

> 你对cache怎么理解的

1. cpu cache
2. page cache

> uma机器的l1 l2 l3的区别? l1和l2有差异吗?  

uma：cpu共享物理内存
numa：每个cpu都有自己的物理内存，访问自己的物理内存较快，访问其他cpu的物理内存较慢

l1 l2 l3的差异：
1. 速度、价格和容量
2. l1分为指令和数据缓存，l2和l3则部分
3. l1和l2在每个cpu核中，l3则是所有cpu共享的
4. linux针对l2会有个cache color算法

![](https://gitee.com/chengshuyi/scripts/raw/master/img/20200809143259.png)

>  理解超线程概念吗?l2是超线程之间可以共享的

超线程的基础是在物理cpu核心之上 虚拟出来的两个逻辑cpu核心，两颗逻辑cpu核心**共享执行资源**，包括：执行引擎，高速缓存，以及系统总线。最重要的是两个逻辑核心各自有一套自己的线程状态存储设施：控制寄存器，通用寄存器。从而调度器可以同时调度两个线程，这个是关键。**最终这么做的目的是充分利用执行引擎**。比如：一个线程在执行定点数运算时，另外一个线程可以同时执行浮点数运算。


>  磁盘的page cache回收时怎么回收(lru,回收的时机?)

1. 页高速缓存（cache）是Linux 内核实现磁盘缓存。它主要用来减少对磁盘的I/O操作。具体地讲，是通过把磁盘中的数据缓存到物理内存中，把对磁盘的访问变为对物理内存的访问。
2. 写缓存策略：写直通法、回写法（linux目前采用的）；
3. 缓存回收：LRU、双链策略（LRU/n）
4. 时机（回收的是干净页，所以首先对脏页进行回写）[这些工作由内核flusher线程完成]：
	a. 空闲内存低于一个特定的阈值时
	b. 当脏页在内存中驻留的时间超过一个特定的阈值
	c. 用户进程调用`sync()`和`fsync`

>  linux内存去向哪里?(应该通过某个命令行查看系统统计)

查看整个系统的：`cat /proc/meminfo`
查看某个进程的：`cat /proc/<pid>/maps`

>  slab和伙伴系统是什么关系

**slab：**
在linux内核中伙伴系统用来管理物理内存，其分配的基本单位是页。由于伙伴系统分配的粒度又太大，因此linux采用slab分配器提供动态内存的管理功能，而且可以作为**经常分配并释放的对象的高速缓存**。slab分配器的优点：

1. 可以提供**小块内存**的分配支持，通用高速缓存可分配的大小从到，专用高速缓存没有大小限制；
2. 不必每次申请释放都和伙伴系统打交道，提供了分配释放效率；
3. 如果在slab缓存的话，其在CPU高速缓存的概率也会较高；
4. 伙伴系统对系统的数据和指令高速缓存有影响，slab分配器采用着色降低了这种副作用；
5. `cat /proc/slabinfo`可以查看slab的分配信息。

**伙伴系统**

1. `cat /proc/buddyinfo`可以查看伙伴系统的分配信息。

>  怎么使用slab

1. 通用缓存：`kmalloc`
2. 高速缓存：`kmem_cache`结构表示一个高速缓存，该结构包含三个链表：`slabs_full`、`slabs_partial`和`slabs_empty`；

> kmalloc和kvmalloc和vmalloc

1. `kmalloc`保证分配的内存在物理上是连续的保证分配的内存在物理上是连续的；
2. `vmalloc`保证的是在虚拟地址空间上的连续，开销更大；
3. `kvmalloc`先调用`kmalloc`，如果失败在调用`vmalloc`。

```c
//下面是kvmalloc代替kmalloc和vmalloc的样例
 static void *seq_buf_alloc(unsigned long size)
 {
-	void *buf;
-	gfp_t gfp = GFP_KERNEL;
-
-	/*
-	 * For high order allocations, use __GFP_NORETRY to avoid oom-killing -
-	 * it's better to fall back to vmalloc() than to kill things.  For small
-	 * allocations, just use GFP_KERNEL which will oom kill, thus no need
-	 * for vmalloc fallback.
-	 */
-	if (size > PAGE_SIZE)
-		gfp |= __GFP_NORETRY | __GFP_NOWARN;
-	buf = kmalloc(size, gfp);
-	if (!buf && size > PAGE_SIZE)
-		buf = vmalloc(size);
-	return buf;
+	return kvmalloc(size, GFP_KERNEL);
 }
```

> 做过哪些驱动,什么类型的



> 自己写一个字符驱动的需要哪些步骤



> 上下文切换什么时候需要进行切换



> 上下文切换大概需要多少时间



> io uring做了什么改进

1. io_uring 为了避免在提交和完成事件中的内存拷贝，设计了一对共享的 ring buffer 用于应用和内核之间的通信。其中，针对提交队列（SQ），应用是 IO 提交的生产者（producer），内核是消费者（consumer）；反过来，针对完成队列（CQ），内核是完成事件的生产者，应用是消费者；
2. 异步化就是不阻塞当前进程的上下文，所以只要能够使用其他线程来操作 buffer io 的内存拷贝等核心操作，我认为也是可以支持的。Linux aio 没有这么做，我理解是这么做效率不高，同时还是有可能因为等待线程池资源而阻塞。io_uring 最新版本已经优化了buffer；

> io uring对上下文切换有什么影响吗?怎么减少切换次数



> io uring在哪个版本支持



> 更换内核吗



> 协程有了解过吗



> 协程一般用在什么场合



> 协程和线程有什么优势



> 虚拟机迁移


> kvm qemu k8s docker


> bio起什么作用

1. bio结构体表示内核中I/O操作；

> 文件系统有了解吗

> 性能优化有了解吗

> ebpf怎么看

> 写过bcc工具吗

> libbpf是干什么的

eBPF 的易用性方面涉及很多问题：
1. 兼容性问题，涉及到对内核数据结构的依赖，导致编写的BPF程序无法复用，容易不工作。
2. BPF是一种字节码，类似汇编语言，较难使用，需要高层次的语言来方便编写。
3. BPF要执行，需要一系列的动作，包括编写代码，加载进内核，从内核读取结果数据，卸载退出等步骤。

BCC：
1. 针对这些问题，BCC（bpf compiler collection）这个工具集专门出现来弥补这些问题
2. BCC编写程序采用c的子集作为前端
3. 编写完毕后自动调用clang进行编译
4. llvm生成ebpf字节码
5. 加载程序到内核
6. 程序退出时自动从内核卸载bpf程序

BCC还有各种前端语言辅助进行变成，前端语言主要是用户态用来处理加载到内核态bpf程序的输出和交互。python支持较好，也支持go。
但是兼容性问题并不好，首先，bcc是类似一种动态语言的方式，每次执行都会进行编译，编译需要工具链和依赖内核头文件（需要安装kernel-header包），而编译依赖是最脆弱，容易失败的。

libbpf：
1. libbpf的出现，目标是为了使得bpf程序像其它程序一样，编译好后，可以放在任何一台机器，任何一个kernel版本上运行（当然要对内核版本有一些要求）。
2. 头文件的问题，依赖内核态特性支持BTF，将内核的数据结构类型构建在内核中。用户态的程序可以导出BTF成一个单独的.h 头文件，bpf程序只要依赖这个头文件就行，不需要安装内核头文件的包了。
3. 兼容性问题，使用clang-11的针对ebpf后端专门的特性：preserve_access_index，支持记录数据结构field相对位置信息，可实现relocation，从而解决了数据结构在不同内核版本间的变化带来的无法对应问题。
4. 性能提升：内核中bpf模块做了一些增强，bpf verifier支持直接字段访问，不需要call bpf函数的方式来访问结构体字段，这样提升了性能。

可以看到，这里其实libbpf是在运行时，对之前编译好的bpf程序做了进一步针对运行平台的处理，即绑定到运行时的操作。而这个操作是依赖内核的BTF支持，是比较轻量的，对外部依赖较小，因而可移植性较好。

35. system tab也可以加一些hook



36. system tab和bpf有什么相比



> bcc用python有什么缺陷吗

bcc：bpf compiler collection

> 调度这一块


> tcp网络协议栈和网络基础知识

**通过socket发送数据：**

应用层：将数据传输给tcp/udp层
1. `write(socket,buffer,length)` 或者 `writev(socket,iovector,vectorlen)`
2. 系统调用`sock_sendmsg`
3. `__sock_sendmsg`根据`proto_ops`调用`tcp_sendmsg`或者`udp_sendmsg`

tcp/udp层：拷贝数据、添加头部信息（序列号）、流量控制和拥塞控制
1. 创建`struct sk_buff`，拷贝用户数据到内核空间
2. `tcp_transmit_skb`添加`tcp`或者`udp`头部、序列号等信息
3. `queue_xmit`：将`sk_buff`加入到发送队列

ip层：添加ip头、维护路由表、TTL、分段
1. ip层首先添加ip头，然后检查路由表和维护TTL，如果发送到网络则调用`ip_queue_xmit`，否则将该数据转发到上层协议

数据链路层：添加以太网头部、arp
2. `dev_queue_xmit`

物理层：网卡驱动

> cgroup有了解吗

cgroups 是Linux内核提供的一种可以限制单个进程或者多个进程所使用资源的机制，可以对 cpu，内存等资源实现精细化的控制，目前越来越火的轻量级容器 Docker 就使用了 cgroups 提供的资源限制能力来完成cpu，内存等部分的资源控制。

另外，开发者也可以使用 cgroups 提供的精细化控制能力，限制某一个或者某一组进程的资源使用。比如在一个既部署了前端 web 服务，也部署了后端计算模块的八核服务器上，可以使用 cgroups 限制 web server 仅可以使用其中的六个核，把剩下的两个核留给后端计算模块。

> 中断 系统调用  可延迟函数 做系统资源互斥

采用自旋锁。

1. 在单cpu，不可抢占内核中，自旋锁为空操作。
2. 在单cpu，可抢占内核中，自旋锁实现为“禁止内核抢占”，并不实现“自旋”。
3. 在多cpu，可抢占内核中，自旋锁实现为“禁止内核抢占” + “自旋”。

NOTE：禁止内核抢占只是关闭“可抢占标志”，而不是禁止进程切换。显式使用schedule或进程阻塞（此也会导致调用schedule）时，还是会发生进程调度的。

> 中断上下部分

中断会打断内核中进程的正常调度和运行，调用中断处理函数。在大多数的系统中，当中断到来时可能耗费大量的时间对它进行处理。如果在中断处理函数中没有禁止中断，该中断处理函数执行过程中仍有可能被其他中断打断。所以我们当然希望中断处理函数执行得越快越好。为了在中断执行**时间尽可能短**的和**中断需要处理完大量的工作**之间找一个平衡点，将中断分为上下两部分。

1. 顶半部完成尽可能少比较紧急的任务，它往往是简单的**读取寄存器中的中断状态**并**清除中断标志**后就进行“**登记中断**”的工作。登记中断：也就是将底半部处理程序挂在底半部执行队列中去。这样上半部执行的速度就会很快，可以服务更多的中断请求。

2. 现在，中断处理工作的重心就落在了底半部的头上，它来完成中断事件的绝大多数任务。底半部几乎做了中断处理程序所有的事情，而且可以被新的中断打断，这也是底半部和顶半部的最大不同，因为顶半部往往被设计成不可中断。底半部则相对来说并不是非常紧急的，而且相对比较耗时，不在硬件中断服务程序中执行。 

> 中断下部分一般怎么做? 软中断 tasklet(可延迟函数) workqueue

可延迟函数：软中断和tasklet(tasklet是基于软中断之上实现的)；

**软中断：**

1. 产生后并不是马上可以执行，必须要等待内核的调度才能执行。软中断不能被自己打断，只能被硬件中断打断（上半部）（中断优先级的关系）;
2. 可以并发运行在多个CPU上（即使同一类型的也可以）。所以软中断必须设计为可重入的函数（允许多个CPU同时操作），因此也需要使用**自旋锁**来保护其数据结构。

**tasklet：**

由于软中断必须使用可重入函数，这就导致设计上的复杂度变高，作为设备驱动程序的开发者来说，增加了负担。而如果某种应用并不需要在多个CPU上并行执行，那么软中断其实是没有必要的。因此诞生了弥补以上两个要求的tasklet。它具有以下特性：
1. **一种特定类型**的tasklet只能**运行在一个CPU上**，不能并行，只能串行执行;
2. **多个不同类型**的tasklet可以并行在多个CPU上;
3. 软中断是静态分配的，在内核编译好之后，就不能改变。但tasklet就灵活许多，可以在**运行时改变**（比如添加模块时）。

```c
// linux内核目前使用的软中断
// /kernel/softirq.c
const char * const softirq_to_name[NR_SOFTIRQS] = {
	"HI", "TIMER", "NET_TX", "NET_RX", "BLOCK", "IRQ_POLL",
	"TASKLET", "SCHED", "HRTIMER", "RCU"
};

// HI：处理高优先级的tasklet
// TIMER：和时钟中断相关的tasklet
// NET_TX：把数据包传送到网卡
// BLOCK：
// TASKLET：处理常规的tasklet
// SCHED
// HRTIMER：高精度定时器中断
// RCU

// tasklet动态使用
void tasklet_init(struct tasklet_struct *t, void (*func)(unsigned long), unsigned long data)
```

**workqueue：**

1. **可延迟函数运行在中断上下文**中（软中断的一个检查点就是do_IRQ退出的时候），于是导致了一些问题：软中断不能睡眠、不能阻塞。由于中断上下文出于内核态，没有进程切换，所以如果软中断一旦睡眠或者阻塞，将无法退出这种状态，导致内核会整个僵死。但可阻塞函数不能用在中断上下文中实现，必须要运行在进程上下文中，例如访问磁盘数据块的函数。因此，可阻塞函数不能用软中断来实现。但是它们往往又具有可延迟的特性。
2. 在2.6版的内核中出现了在内核态运行的**工作队列（运行在进程上下文）**（替代了2.4内核中的任务队列）。它也具有一些可延迟函数的特点（需要被激活和延后执行），但是能够能够在不同的进程间切换，以完成不同的工作。可延迟函数：软中断和tasklet(tasklet是基于软中断之上实现的)；
